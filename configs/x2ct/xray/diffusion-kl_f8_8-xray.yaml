model:
  base_learning_rate: 1.0e-5
  target: sgm.models.diffusion.DiffusionEngine
  params:
    scale_factor: 0.13025
    disable_first_stage_autocast: True
    input_key: xray

    scheduler_config:
      target: sgm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [10000]
        cycle_lengths: [10000000000000]
        f_start: [1.e-5]
        f_max: [1.]
        f_min: [1.]

    denoiser_config:
      target: sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser
      params:
        num_idx: 1000

        scaling_config:
          target: sgm.modules.diffusionmodules.denoiser_scaling.EpsScaling
        discretization_config:
          target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

    network_config:
      target: sgm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        use_checkpoint: True
        in_channels: 8
        out_channels: 8
        model_channels: 320
        attention_resolutions: [4, 2, 1]
        num_res_blocks: 2
        channel_mult: [1, 2, 4, 4]
        num_head_channels: 64
        use_linear_in_transformer: True
        transformer_depth: 1
        # context_dim: 1024
        spatial_transformer_attn_type: softmax-xformers

    first_stage_config:
      target: sgm.models.autoencoder.AutoencodingEngine
      params:
        input_key: xray
        monitor: val/loss/rec
        ckpt_engine: /raid/maximilian.schulze/generative-models/logs/2025-02-13T19-45-44_x2ct-xray-imagenet-kl_f8_8chn-xray/checkpoints/epoch=000968.ckpt

        encoder_config:
          target: sgm.modules.diffusionmodules.model.Encoder
          params:
            attn_type: vanilla-xformers
            double_z: true
            z_channels: 8
            resolution: 256
            in_channels: 1
            out_ch: 1
            ch: 128
            ch_mult: [1, 2, 4, 4]
            num_res_blocks: 2
            attn_resolutions: []
            dropout: 0.0

        decoder_config:
          target: sgm.modules.diffusionmodules.model.Decoder
          params: ${model.params.first_stage_config.params.encoder_config.params}

        regularizer_config:
          target: sgm.modules.autoencoding.regularizers.DiagonalGaussianRegularizer

        loss_config:
          target: torch.nn.Identity

    loss_fn_config:
      target: sgm.modules.diffusionmodules.loss.StandardDiffusionLoss
      params:
        loss_weighting_config:
          target: sgm.modules.diffusionmodules.loss_weighting.EpsWeighting
        sigma_sampler_config:
          target: sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling
          params:
            num_idx: 1000

            discretization_config:
              target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

    sampler_config:
      target: sgm.modules.diffusionmodules.sampling.EulerEDMSampler
      params:
        num_steps: 50

        discretization_config:
          target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

        guider_config:
          target: sgm.modules.diffusionmodules.guiders.VanillaCFG
          params:
            scale: 7.5

data:
  target: sgm.data.radchest.RadchestIndividualXrayDataloader
  params:
    num_projections: 2
    train:
      loader:
          batch_size: 64
          num_workers: 8
      dataset:
        target: sgm.data.radchest.CachedXCT_H5_dataset
        params:
          cache_dir: /raid/maximilian.schulze/dataset_cache
          optimize_zip: False
          h5_dataset_path: /raid/shared/x2ct/radchest/h5/sitk-dataset.h5
          h5_ct_group_path: 512/ct
          h5_xray_group_path: 512/drr
          h5_text_dataset_path: /raid/shared/x2ct/radchest/discription-latents-h5/short-template.h5
          h5_text_group_path: BiomedCLIPTextEmbedder/V2/OneNonNegativeTokens
          split_file: /raid/shared/x2ct/radchest/train_val_split.json
          train: True
          scale: 256
          scale_images: True
          crop_size: 256
          use_f16: False
          ct_only: False
          xray_only: True
          load_text: True
          num_projections: 2
          is_preprocessed: False
    validation:
      loader:
        batch_size: 64
        num_workers: 8
      dataset:
        target: sgm.data.radchest.CachedXCT_H5_dataset
        params:
          cache_dir: /raid/maximilian.schulze/dataset_cache
          optimize_zip: False
          h5_dataset_path: /raid/shared/x2ct/radchest/h5/sitk-dataset.h5
          h5_ct_group_path: 512/ct
          h5_xray_group_path: 512/drr
          h5_text_dataset_path: /raid/shared/x2ct/radchest/discription-latents-h5/short-template.h5
          h5_text_group_path: BiomedCLIPTextEmbedder/V2/OneNonNegativeTokens
          split_file: /raid/shared/x2ct/radchest/train_val_split.json
          train: False
          scale: 256
          scale_images: True
          crop_size: 256
          use_f16: False
          ct_only: False
          xray_only: True
          load_text: True
          num_projections: 2
          is_preprocessed: False

lightning:
  # modelcheckpoint:
  #   params:
  #     every_n_train_steps: 1000

  # callbacks:
  #   metrics_over_trainsteps_checkpoint:
  #     params:
  #       every_n_train_steps: 1000

  modelcheckpoint:
    params:
      every_n_train_steps: 5000

  callbacks:
    metrics_over_trainsteps_checkpoint:
      params:
        every_n_train_steps: 10000

    image_logger:
      target: main.ImageLogger
      params:
        disabled: False
        batch_frequency: 1000
        max_images: 64
        increase_log_steps: False
        log_first_step: False
        log_images_kwargs:
          use_ema_scope: False
          N: 64
          n_rows: 8

  trainer:
    devices: 0,
    benchmark: True
    # num_sanity_val_steps: 1
    accumulate_grad_batches: 1
    max_epochs: 1000
    precision: bf16
